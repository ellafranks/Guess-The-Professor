{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:14:18.524575Z",
     "start_time": "2020-05-01T09:13:55.654067Z"
    }
   },
   "source": [
    "A persons writing style is an example of behaviour biometric. The words people may use and the way they strcuture their scentacnes is distinctive and can often be used to identify the author of a particular work. \n",
    "\n",
    "In this project I aim to compare two economists and classify who is the primary author of an academic paper based off their language. \n",
    "\n",
    "To ensure a large corpus, I will use Selinium to scrape Google Scholar, one of the most used academic search engines in the world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:46:46.794864Z",
     "start_time": "2020-05-07T08:46:46.790039Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:46:41.965476Z",
     "start_time": "2020-05-07T08:46:41.959942Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:55:48.165410Z",
     "start_time": "2020-05-07T08:55:48.139843Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:55:49.751340Z",
     "start_time": "2020-05-07T08:55:48.503470Z"
    }
   },
   "outputs": [],
   "source": [
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:55:52.621461Z",
     "start_time": "2020-05-07T08:55:49.759270Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:59:27.283285Z",
     "start_time": "2020-05-07T09:59:27.276794Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:56:45.421632Z",
     "start_time": "2020-05-07T09:56:45.411112Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:56:45.683195Z",
     "start_time": "2020-05-07T09:56:45.677145Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:04:34.155941Z",
     "start_time": "2020-05-07T10:04:34.147552Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:05:38.040996Z",
     "start_time": "2020-05-07T10:05:38.036162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Collect and download pdfs from google scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Main caveats of google scholar: \n",
    "- Cannot perform large scale analysis due to CAPTCHAS (robots) \n",
    "- Google scholar search enginge includes mentions of prof as well as author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:43:49.675849Z",
     "start_time": "2020-05-07T08:43:49.666136Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def open_and_search(prof, sleep, directory):\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": directory, #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True, #It will not show PDF directly in chrome\n",
    "    \"plugins.extensions_to_open\": \"\" \n",
    "    }) \n",
    "    \n",
    "    driver = webdriver.Chrome('/Users/ellafranks/Downloads/chromedriver', options=options)\n",
    "    driver.get('https://scholar.google.com/')\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    driver.find_element_by_id('gs_hdr_tsi').send_keys(prof)\n",
    "\n",
    "    driver.find_element_by_name('btnG').click()\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    return driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:43:50.064947Z",
     "start_time": "2020-05-07T08:43:50.058065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def click_citation(driver, prof):\n",
    "    time.sleep(1)\n",
    "    driver.get(\"https://scholar.google.com/scholar?as_vis=1&q={}&hl=en&as_sdt=0,5\".format(prof))\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:43:50.526045Z",
     "start_time": "2020-05-07T08:43:50.516672Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def download_pdf(driver, sleep, num_pages, prof, name):\n",
    "    \n",
    "    for i in [i*10 for i in range(num_pages)]: \n",
    "  \n",
    "        driver.get('https://scholar.google.com/scholar?start={}&q={}&hl=en&as_sdt=0,5&as_vis=1'.format(i, prof))\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        blocks = driver.find_elements_by_class_name('gs_r.gs_or.gs_scl')\n",
    "\n",
    "        for block in blocks:\n",
    "            try: \n",
    "                if block.find_element_by_class_name('gs_or_ggsm'):\n",
    "                    if name in block.find_element_by_class_name('gs_a').text:\n",
    "                        block.find_element_by_class_name('gs_ctg2').click()\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**This next line of code is made up of 3 functions:**\n",
    "\n",
    "1. Navigate to google scholar page, search up associated prof name\n",
    "2. Apply filters ensuring prof is a key author and not just a citation\n",
    "3. Download papers that are pdf formatted AND associated prof name as primary author into chosen directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These two prof were chosen due to them having written papers with similar content, but never coauthoring a paper together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:43:52.241552Z",
     "start_time": "2020-05-07T08:43:52.236710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "directory1 = \"/Users/ellafranks/Desktop/Other-projects/Guess_the_prof/Julian_Franks_papers/\"\n",
    "directory2 = \"/Users/ellafranks/Desktop/Other-projects/Guess_the_prof/La_Porta_papers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:44:23.314213Z",
     "start_time": "2020-05-07T08:44:23.308906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Prof 1 - Julian Franks\n",
    "download_pdf(click_citation(open_and_search('Julian Franks', 1, directory1), 'Julian+Franks'), 1, 20, 'Julian+Franks', 'Franks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:07:28.956387Z",
     "start_time": "2020-05-06T14:07:28.951194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Prof 2 - Rafael La Porta\n",
    "download_pdf(click_citation(open_and_search('La Porta', 1, directory2), 'La+Porta'), 1, 20, 'La+Porta', 'Porta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parse and convert pdf to raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using Tika to extract content from pdfs (need to install the latest version of Java in order to run the script - Can use pdfMiner3 or pdfPlumber if not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:10:24.012181Z",
     "start_time": "2020-05-06T12:10:23.987829Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def grab_pdf_content(filepath):\n",
    "    \n",
    "    list_pdf, input_files = [], []\n",
    "    \n",
    "    #grab all PDF's file name and strip out .pdf\n",
    "    all_files = glob.glob(os.path.join(filepath, \"*.pdf\"))\n",
    "    for input_file in all_files:\n",
    "        filename = os.path.basename(input_file)\n",
    "        filename = filename.strip('.pdf')\n",
    "        \n",
    "    # Use Tika to parse the PDF's, extract text content\n",
    "        parsedPDF = parser.from_file(input_file)\n",
    "        pdf = parsedPDF[\"content\"]\n",
    "        \n",
    "        list_pdf.append(pdf)\n",
    "        input_files.append(filename)\n",
    " \n",
    "    return input_files, list_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:10:26.386327Z",
     "start_time": "2020-05-06T12:10:26.377054Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert_to_df(n, c): \n",
    "    corpus = list(zip(n, c))\n",
    "    df = pd.DataFrame.from_dict({name:doc for name,doc in corpus}, orient='index', columns=['text'])\n",
    "    df.to_csv('{}_pdfs_df.csv'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:07:37.510036Z",
     "start_time": "2020-05-06T14:07:37.481110Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filepath = \"/../Users/ellafranks/Desktop/Other-projects/Guess_the_prof/Julian_Franks_papers\"\n",
    "filepath2 = \"/../Users/ellafranks/Desktop/Other-projects/Guess_the_prof/La_Porta_papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Extract filename and content from Julian Franks\n",
    "2. Compile and save pdfs into a csv dataframe \n",
    "3. Add relevent column (prof) and combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T13:31:07.860476Z",
     "start_time": "2020-05-06T13:31:07.846896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Julian Franks\n",
    "name, content = grab_pdf_content(filepath)\n",
    "convert_to_df(name, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:07:56.375618Z",
     "start_time": "2020-05-06T14:07:39.708292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# La Porta\n",
    "name2, content2 = grab_pdf_content(filepath2)\n",
    "convert_to_df(name2, content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:56:58.148331Z",
     "start_time": "2020-05-07T08:56:57.922130Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "JF = pd.read_csv('38_pdfs_df.csv', sep=',', index_col=[0])\n",
    "LP = pd.read_csv('59_pdfs_df.csv', sep=',', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:56:58.370580Z",
     "start_time": "2020-05-07T08:56:58.356783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "JF['prof'] = 'Julian'\n",
    "LP['prof'] = 'Rafael'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:43:22.310955Z",
     "start_time": "2020-05-07T09:43:22.299977Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb = pd.concat([JF, LP])\n",
    "comb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clean and prepare text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "MORE RULES?\n",
    "\n",
    "- REMOVE REFERENCES?\n",
    "- REMOVE FIGURES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:43:23.464122Z",
     "start_time": "2020-05-07T09:43:23.457374Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def general_clean(content):\n",
    "    paragraphs = str(content).split('\\n\\n')\n",
    "    list_stripped = []  \n",
    "    for item in paragraphs:\n",
    "        stripped = item.strip().replace(',', '').lower()\n",
    "        list_stripped.append(stripped)\n",
    "    \n",
    "    #remove if empty cell or paragraph break \n",
    "    final_list = []\n",
    "    for l in list_stripped:\n",
    "        if l != '' and '\\n' not in l:\n",
    "            final_list.append(l)  \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Above I split the content by newline paragraphs, strip them and convert to lowercase and then remove the cell if empty or contains a paragraph break. I then 'explode' the list of lists per row into many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:43:24.601942Z",
     "start_time": "2020-05-07T09:43:24.458762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comb.text = comb.text.apply(lambda x: general_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:43:25.011171Z",
     "start_time": "2020-05-07T09:43:24.974409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = comb.explode('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:44:10.066538Z",
     "start_time": "2020-05-07T09:44:10.031094Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10001 duplictates found in 49564 rows.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} duplictates found in {} rows.'.format(df.duplicated().sum(), df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I have a very large corpus and in order to make this clean as applicable to all pdfs and their authors, I drop duplicate cells which mostly contain page numbers and chapter references. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:44:23.057107Z",
     "start_time": "2020-05-07T09:44:23.024568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:44:23.542047Z",
     "start_time": "2020-05-07T09:44:23.475884Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will drop 13438 rows which are less than 10 words long.\n"
     ]
    }
   ],
   "source": [
    "print('I will drop {} rows which are less than 10 words long.'.format(df.text.apply(lambda x: len(x.split(' ')) < 10).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:44:25.209635Z",
     "start_time": "2020-05-07T09:44:25.143534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df['text'].apply(lambda x: len(x.split(' ')) > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:44:25.704560Z",
     "start_time": "2020-05-07T09:44:25.685501Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index().rename(columns={'index': 'paper'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Remove all occurances of prof name and all instances where cells begin with an asterix as they tend to be footnotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:44:30.268791Z",
     "start_time": "2020-05-07T09:44:30.041760Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6924 number of instances of the prof name, so we shall replace these.\n"
     ]
    }
   ],
   "source": [
    "prof = 'Julian|Franks|Rafael|Porta|La'\n",
    "\n",
    "print('There are {} number of instances of the prof name, so we shall replace these.'.\\\n",
    "      format(df[df['text'].str.contains(prof, flags=re.IGNORECASE)].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:46:17.181551Z",
     "start_time": "2020-05-07T09:46:16.975089Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prof = ['julian', 'franks', 'rafael', 'porta', 'la']\n",
    "df.text = df.text.replace({l: '' for l in prof}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:53:29.226855Z",
     "start_time": "2020-05-07T09:53:29.205736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.text.map(lambda x: not (x.startswith('*')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We would ideally like the classes as balenced as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:53:50.211444Z",
     "start_time": "2020-05-07T09:53:50.181629Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prof</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Julian</th>\n",
       "      <td>15311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rafael</th>\n",
       "      <td>9166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "prof         \n",
       "Julian  15311\n",
       "Rafael   9166"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('prof')[['text']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process text using spacy and make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a classification model to predict if the economics paper was Julian Franks or La Porta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:10:10.427733Z",
     "start_time": "2020-05-07T10:10:10.406826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6255260040037587"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prof'].value_counts(normalize=True).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngrams, max features\n",
    "- remove dots, punctuation, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Creating our tokenizer function that returns a list of preproccessed tokens that are lemmatised, lowercased and have had stopwords removed.**\n",
    "\n",
    "- **Custom transformer using spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:14:40.734324Z",
     "start_time": "2020-05-07T10:14:40.490464Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "stop_words = STOP_WORDS\n",
    "parser = English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    return mytokens\n",
    "\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:19:12.000309Z",
     "start_time": "2020-05-07T11:19:11.871892Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['prof'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:19:20.095507Z",
     "start_time": "2020-05-07T11:19:12.447854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x1a2d905dd8>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ng...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "\n",
    "classifier = LogisticRegression(solver = 'lbfgs')\n",
    "\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:19:22.950256Z",
     "start_time": "2020-05-07T11:19:21.514045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9107434640522876\n"
     ]
    }
   ],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:19:29.198550Z",
     "start_time": "2020-05-07T11:19:29.030213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>government</td>\n",
       "      <td>6.472846</td>\n",
       "      <td>6.472846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10668</th>\n",
       "      <td>nodes</td>\n",
       "      <td>4.158441</td>\n",
       "      <td>4.158441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>mobile</td>\n",
       "      <td>4.044718</td>\n",
       "      <td>4.044718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>legal</td>\n",
       "      <td>4.040153</td>\n",
       "      <td>4.040153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8436</th>\n",
       "      <td>human</td>\n",
       "      <td>3.995545</td>\n",
       "      <td>3.995545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>-4.337710</td>\n",
       "      <td>4.337710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>uk</td>\n",
       "      <td>-4.655064</td>\n",
       "      <td>4.655064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14605</th>\n",
       "      <td>u.k</td>\n",
       "      <td>-4.893694</td>\n",
       "      <td>4.893694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266</th>\n",
       "      <td>companies</td>\n",
       "      <td>-5.264934</td>\n",
       "      <td>5.264934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7396</th>\n",
       "      <td>family</td>\n",
       "      <td>-5.446064</td>\n",
       "      <td>5.446064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15757 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature      coef  abs_coef\n",
       "8028   government  6.472846  6.472846\n",
       "10668       nodes  4.158441  4.158441\n",
       "10342      mobile  4.044718  4.044718\n",
       "9536        legal  4.040153  4.040153\n",
       "8436        human  3.995545  3.995545\n",
       "...           ...       ...       ...\n",
       "4169   bankruptcy -4.337710  4.337710\n",
       "14637          uk -4.655064  4.655064\n",
       "14605         u.k -4.893694  4.893694\n",
       "5266    companies -5.264934  5.264934\n",
       "7396       family -5.446064  5.446064\n",
       "\n",
       "[15757 rows x 3 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'feature': pipe.named_steps['vectorizer'].get_feature_names(),\n",
    "                        'coef': pipe.named_steps['classifier'].coef_[0],\n",
    "                        'abs_coef': np.abs(pipe.named_steps['classifier'].coef_[0])}).sort_values('coef', ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:19:34.096792Z",
     "start_time": "2020-05-07T11:19:34.024454Z"
    }
   },
   "outputs": [],
   "source": [
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "def tokenizeText(sample):\n",
    "    tokens = parser(sample)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:19:38.345330Z",
     "start_time": "2020-05-07T11:19:38.305315Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:21:25.860711Z",
     "start_time": "2020-05-07T11:21:25.837475Z"
    }
   },
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "        \n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "# data\n",
    "train1 = train['text'].tolist()\n",
    "labelsTrain1 = train['prof'].tolist()\n",
    "test1 = test['text'].tolist()\n",
    "labelsTest1 = test['prof'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T11:21:40.117285Z",
     "start_time": "2020-05-07T11:21:26.638651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9268790849673203\n",
      "Top 10 features used to predict: \n",
      "Class 1 best: \n",
      "(-2.648986744274183, 'uk')\n",
      "(-2.3825863401634937, 'u.k')\n",
      "(-2.2774710313825546, 'companies')\n",
      "(-2.1908991429411535, 'bankruptcy')\n",
      "(-2.147959857170371, 'family')\n",
      "(-2.106737782657793, 'repurchases')\n",
      "(-1.9652703665389022, 'innovation')\n",
      "(-1.9470774719221438, 'bias')\n",
      "(-1.9464173180338016, 'engagement')\n",
      "(-1.8067363155920442, 'acquisitions')\n",
      "(-1.7878129624404822, 'turnover')\n",
      "(-1.78555150325406, 'activism')\n",
      "(-1.7607516085203698, 'lenders')\n",
      "(-1.7201163894887195, 'diverged')\n",
      "(-1.7147090266210705, 'outsider')\n",
      "(-1.70986213512913, '1900')\n",
      "(-1.685331902608087, 'liquidation')\n",
      "(-1.6779837263859279, 'active')\n",
      "(-1.6616356304353777, 'fund')\n",
      "(-1.65405984315486, 'chapter')\n",
      "Class 2 best: \n",
      "(3.0572213109157027, '')\n",
      "(2.944518022800794, 'government')\n",
      "(2.7561543408867952, 'schooling')\n",
      "(2.5719590912360517, 'bor')\n",
      "(2.520709738660581, 'human')\n",
      "(2.4536669839294225, 'expropriation')\n",
      "(2.370010575927269, 'key')\n",
      "(2.33555475088195, 'protocol')\n",
      "(2.297628697967669, 'centered')\n",
      "(2.291162160467385, 'node')\n",
      "(2.2418470027712787, 'nodes')\n",
      "(2.212706041212838, 'sensors')\n",
      "(2.1882706413555866, 'keys')\n",
      "(2.1858881374355685, 'workers')\n",
      "(2.1753978269494967, 'regutions')\n",
      "(2.174071790158945, 'dealing')\n",
      "(2.1711234550291536, 'worker')\n",
      "(2.1632584675024424, 'civil')\n",
      "(2.163250313959687, 'network')\n",
      "(2.137966996311501, 'constitutional')\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "pipe.fit(train1, labelsTrain1)\n",
    "# test\n",
    "preds = pipe.predict(test1)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\n",
    "print(\"Top 10 features used to predict: \")\n",
    "\n",
    "printNMostInformative(vectorizer, clf, 20)\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train1, labelsTrain1)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "for i in range(len(train1)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textacy",
   "language": "python",
   "name": "textacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
